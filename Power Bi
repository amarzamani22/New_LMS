# app.py
import os
import re
import numpy as np
import pandas as pd
import streamlit as st
import plotly.graph_objects as go

# =========================
# Page & Theme
# =========================
st.set_page_config(page_title="QC Intelligence Console", page_icon="ðŸ“Š", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #F5F7FB; }
    .kpi-card {
        padding: 16px; border-radius: 10px; background: #fff; border: 1px solid #e7e7e7;
        box-shadow: 0 2px 8px rgba(0,0,0,0.03); text-align:center;
    }
    .subtle { color:#666; font-size:0.9rem; }
</style>
""", unsafe_allow_html=True)

# =========================
# Constants
# =========================
ALL_MONTHS = ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']
SHEET_MAP = {
    'Q1A: Employees': 'QC_Q1A_Main',
    'Q2A: Salary': 'QC_Q2A_Main',
    'Q3: Hours Worked': 'QC_Q3',
    'Q4: Vacancies': 'QC_Q4',
    'Q5: Separations': 'QC_Q5'
}

COMPOSITE_WEIGHTS_MONTHLY = dict(MoM=0.35, YoY=0.30, IQR=0.20, ROLL=0.15)
COMPOSITE_WEIGHTS_QUARTERLY = dict(MoM=0.25, YoY=0.40, IQR=0.25, ROLL=0.10)

COMPOSITE_LABELS = [
    (0.75, "True Outlier"),
    (0.65, "Probable Outlier"),
    (0.50, "Watchlist"),
]

# Rolling defaults (exposed in UI)
DEFAULT_ROLL_WIN = 3
DEFAULT_ROLL_KSIG = 2.5
DEFAULT_ROLL_REL = 0.25  # 25%

# =========================
# Data Loaders
# =========================
@st.cache_data
def load_data(year: int|None, sheet_name: str):
    if year is None:
        return None
    file_path = f"submission/qc_workbook_{year}.xlsx"
    if not os.path.exists(file_path):
        return f"Error: file not found â†’ {file_path}"
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, header=5)
        df.dropna(axis=1, how='all', inplace=True)
        df.rename(columns={'Q1.1':'Q1_Total','Q2.1':'Q2_Total','Q3.1':'Q3_Total','Q4.1':'Q4_Total'}, inplace=True)
        return df
    except Exception as e:
        return f"Error reading '{sheet_name}' from {file_path}: {e}"

@st.cache_data
def get_reporting_quarter(year: int):
    file_path = f"submission/qc_workbook_{year}.xlsx"
    try:
        about_df = pd.read_excel(file_path, sheet_name="_About", header=None)
        quarter_row = about_df[about_df[0] == 'Quarter']
        if not quarter_row.empty:
            return int(re.search(r'\d+', str(quarter_row.iloc[0, 1])).group())
    except Exception:
        pass
    return 4

@st.cache_data
def load_hierarchy_map(path="submission/entity_hierarchy.csv"):
    if not os.path.exists(path):
        return None, "Hierarchy map not found. Expect submission/entity_hierarchy.csv with Institution,Sector,Subsector."
    try:
        h = pd.read_csv(path)
        required = {'Institution','Sector','Subsector'}
        if not required.issubset(set(h.columns)):
            return None, f"Hierarchy map missing columns. Need {required}"
        return h, None
    except Exception as e:
        return None, f"Error reading hierarchy map: {e}"

# =========================
# Helpers
# =========================
def _row_filter(df, entity, worker_cat, subq):
    cond = (df['Entity / Group'] == entity) & (df['Worker Category'] == worker_cat)
    if 'Subquestion' in df.columns:
        cond &= (df['Subquestion'] == subq)
    return df[cond]

def build_quarter_series(data_row, year, reporting_quarter):
    if data_row.empty: return pd.Series(dtype=float)
    # Use Qx_Total if present, else compute from months
    labels, vals = [], []
    have_any = False
    for q in range(1, reporting_quarter+1):
        col = f'Q{q}_Total'
        if col in data_row.columns:
            v = pd.to_numeric(data_row.iloc[0][col], errors='coerce')
            if not pd.isna(v):
                have_any = True
                labels.append(f"Q{q} {year}"); vals.append(float(v))
    if not have_any or len(labels) < reporting_quarter:
        q_map = {1: ALL_MONTHS[0:3], 2: ALL_MONTHS[3:6], 3: ALL_MONTHS[6:9], 4: ALL_MONTHS[9:12]}
        present = {int(l.split()[0][1:]) for l in labels}
        for q in range(1, reporting_quarter+1):
            if q in present: continue
            mlist = [m for m in q_map[q] if m in data_row.columns]
            if mlist:
                ser = pd.to_numeric(data_row[mlist].iloc[0], errors='coerce').astype(float)
                if not np.all(np.isnan(ser.values)):
                    labels.append(f"Q{q} {year}")
                    vals.append(float(np.nansum(ser.values)))
    if not labels: return pd.Series(dtype=float)
    # sort by Q number
    order = np.argsort([int(l.split()[0][1:]) for l in labels])
    labels = [labels[i] for i in order]; vals = [vals[i] for i in order]
    return pd.Series(vals, index=labels, dtype=float)

def plot_series(current_series, prior_series, title):
    x = list(current_series.index)
    y = current_series.values.astype(float)
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=x, y=y, mode='lines+markers', name='Current',
        hovertemplate='Period: %{x}<br>Value: %{y:,.0f}<extra></extra>'
    ))
    if prior_series is not None and not prior_series.empty:
        fig.add_trace(go.Scatter(
            x=list(prior_series.index),
            y=prior_series.values.astype(float),
            mode='lines+markers', name='Comparison', line=dict(dash='dash'),
            hovertemplate='Period: %{x}<br>Value: %{y:,.0f}<extra></extra>'
        ))
    fig.update_layout(
        title=title, hovermode='x unified',
        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='left', x=0),
        margin=dict(l=10, r=10, t=50, b=10)
    )
    fig.update_xaxes(tickangle=45)
    st.plotly_chart(fig, use_container_width=True)

# =========================
# 4-Pillar Scoring
# =========================
def _rolling_stats(series, window):
    m = series.rolling(window=window, min_periods=max(2, window//2)).mean()
    s = series.rolling(window=window, min_periods=max(2, window//2)).std()
    return m, s

def score_pillars(series, prior_series, mom_pct_thresh, mom_abs_thresh, yoy_pct_thresh, iqr_k, roll_w, roll_ksig, roll_rel):
    idx = list(series.index)
    out = []

    valid = series.dropna()
    if len(valid) >= 4:
        q1, q3 = valid.quantile(0.25), valid.quantile(0.75)
        iqr = q3 - q1
        lb, ub = q1 - iqr_k*iqr, q3 + iqr_k*iqr
    else:
        lb = ub = iqr = None

    r_mean, r_std = _rolling_stats(series, roll_w)

    for i, p in enumerate(idx):
        v = series.iloc[i]
        reasons = []
        s_mom = s_yoy = s_iqr = s_roll = 0.0

        # MoM
        if i > 0 and not pd.isna(v):
            prev = series.iloc[i-1]
            if not pd.isna(prev) and prev != 0:
                d_abs = v - prev
                d_pct = d_abs / prev
                if abs(d_pct) > mom_pct_thresh and abs(d_abs) > mom_abs_thresh:
                    s_mom = 1.0 if abs(d_pct) > max(0.5, mom_pct_thresh*2) else 0.6
                    reasons.append(f"MoM {d_pct:+.1%} (Î” {d_abs:+,.0f})")

        # YoY
        if prior_series is not None and p in prior_series.index and not pd.isna(v):
            prev_y = prior_series.loc[p]
            if not pd.isna(prev_y) and prev_y != 0:
                yoy = (v - prev_y) / prev_y
                if abs(yoy) > yoy_pct_thresh:
                    s_yoy = 1.0 if abs(yoy) > max(0.6, yoy_pct_thresh*2) else 0.6
                    reasons.append(f"YoY {yoy:+.1%}")

        # IQR
        if lb is not None and not pd.isna(v):
            if v < lb or v > ub:
                # distance approx
                dist = 0.0
                if iqr and iqr != 0:
                    dist = (ub - v)/iqr if v > ub else (v - lb)/iqr
                    dist = abs(dist)
                s_iqr = 1.0 if dist >= 1.0 else 0.6
                reasons.append("IQR outlier")

        # Rolling
        mu = r_mean.iloc[i] if len(r_mean) > i else np.nan
        sd = r_std.iloc[i] if len(r_std) > i else np.nan
        if not pd.isna(v) and not pd.isna(mu):
            if not pd.isna(sd) and sd > 0:
                z = abs(v - mu) / sd
                if z >= 3.0:
                    s_roll = 1.0; reasons.append(f"Rolling {z:.1f}Ïƒ")
                elif z >= roll_ksig:
                    s_roll = 0.6; reasons.append(f"Rolling {z:.1f}Ïƒ")
            else:
                if mu != 0:
                    rel = abs(v - mu)/abs(mu)
                    if rel >= 0.40:
                        s_roll = 1.0; reasons.append(f"Rolling {rel:.0%}")
                    elif rel >= roll_rel:
                        s_roll = 0.6; reasons.append(f"Rolling {rel:.0%}")

        out.append(dict(Period=p, MoM_score=s_mom, YoY_score=s_yoy, IQR_score=s_iqr, ROLL_score=s_roll, Reasons=reasons))
    return pd.DataFrame(out).set_index("Period")

def compose_score(scores_df, view):
    if scores_df.empty:
        return scores_df.assign(Composite=0.0, Label="")
    W = COMPOSITE_WEIGHTS_MONTHLY if view == "Monthly" else COMPOSITE_WEIGHTS_QUARTERLY
    comp = (W['MoM']*scores_df['MoM_score'] + W['YoY']*scores_df['YoY_score'] +
            W['IQR']*scores_df['IQR_score'] + W['ROLL']*scores_df['ROLL_score'])
    pillars = scores_df[['MoM_score','YoY_score','IQR_score','ROLL_score']]
    strong = (pillars >= 0.6).sum(axis=1)
    any1 = (pillars >= 0.999).sum(axis=1) >= 1
    any04 = (pillars >= 0.4).sum(axis=1) >= 2
    consensus = (strong >= 2) | (any1 & any04)
    labels = []
    for c, ok in zip(comp, consensus):
        label = ""
        if ok:
            for thr, name in COMPOSITE_LABELS:
                if c >= thr:
                    label = name; break
        labels.append(label)
    out = scores_df.copy()
    out['Composite'] = comp
    out['Label'] = labels
    return out

# =========================
# Hierarchical Aggregation
# =========================
def aggregate_monthly(df_slice, months):
    work = df_slice.copy()
    for m in months:
        if m in work.columns:
            work[m] = pd.to_numeric(work[m], errors='coerce')
    return work

def build_level_series(df_sheet, cols, hmap):
    merged = df_sheet.merge(hmap, left_on='Entity / Group', right_on='Institution', how='left')
    # L3 institution
    inst = {}
    for name, g in merged.groupby('Institution', dropna=False):
        if name is None or pd.isna(name): continue
        s = g[cols].sum(numeric_only=True); s.index = cols; inst[name] = s
    # L2 subsector
    sub = {}
    for (sec, subsec), g in merged.groupby(['Sector','Subsector'], dropna=False):
        if pd.isna(sec) or pd.isna(subsec): continue
        s = g[cols].sum(numeric_only=True); s.index = cols; sub[(sec, subsec)] = s
    # L1 sector
    secd = {}
    for sec, g in merged.groupby('Sector', dropna=False):
        if pd.isna(sec): continue
        s = g[cols].sum(numeric_only=True); s.index = cols; secd[sec] = s
    # L0 all fi
    allfi = merged[cols].sum(numeric_only=True); allfi.index = cols
    return dict(AllFI=allfi, Sector=secd, Subsector=sub, Institution=inst)

def _delta_prev(series, period):
    if period not in series.index: return np.nan
    i = list(series.index).index(period)
    if i == 0: return np.nan
    v, pv = series.iloc[i], series.iloc[i-1]
    if pd.isna(v) or pd.isna(pv): return np.nan
    return v - pv

def attribute_path(period, agg):
    out = {'Period': period}
    allfi = agg['AllFI']
    out['AllFI_value'] = allfi.get(period, np.nan)
    out['AllFI_delta'] = _delta_prev(allfi, period)

    # sector
    s_contrib = {sec: _delta_prev(s, period) for sec, s in agg['Sector'].items()}
    sec_best = max(s_contrib, key=lambda k: abs(s_contrib[k]) if not pd.isna(s_contrib[k]) else -1) if s_contrib else None
    out['Sector'] = sec_best; out['Sector_delta'] = s_contrib.get(sec_best, np.nan) if sec_best else np.nan

    # subsector within sector
    sub_choices = {k: v for k, v in agg['Subsector'].items() if k[0] == sec_best} if sec_best else {}
    sub_contrib = {sub: _delta_prev(s, period) for sub, s in sub_choices.items()}
    sub_best = max(sub_contrib, key=lambda k: abs(sub_contrib[k]) if not pd.isna(sub_contrib[k]) else -1) if sub_contrib else None
    out['Subsector'] = sub_best[1] if sub_best else None
    out['Subsector_delta'] = sub_contrib.get(sub_best, np.nan) if sub_best else np.nan

    # institution within chosen subsector (strict filter if available)
    inst_contrib = {}
    if sub_best:
        chosen_sec, chosen_sub = sub_best
        # Filter institutions by that subsector via a reverse map is faster, but weâ€™ll select by max overall for simplicity
        inst_contrib = {inst: _delta_prev(s, period) for inst, s in agg['Institution'].items()}
    else:
        inst_contrib = {inst: _delta_prev(s, period) for inst, s in agg['Institution'].items()}

    inst_best = max(inst_contrib, key=lambda k: abs(inst_contrib[k]) if not pd.isna(inst_contrib[k]) else -1) if inst_contrib else None
    out['Institution'] = inst_best
    out['Institution_delta'] = inst_contrib.get(inst_best, np.nan) if inst_best else np.nan
    return out

# =========================
# Sidebar (global)
# =========================
st.sidebar.title("Controls")
current_year = st.sidebar.selectbox("Current Year", [2026, 2025, 2024, 2023], index=1)
comparison_year = st.sidebar.selectbox("Comparison Year", [2025, 2024, 2023, None], index=0)
dataset = st.sidebar.selectbox("Dataset", list(SHEET_MAP.keys()))
time_view = st.sidebar.radio("Timeframe", ['Monthly','Quarterly'], horizontal=True)
st.sidebar.markdown("---")
abs_thresh = st.sidebar.slider("Absolute Change floor", 10, 1000, 50, 10)
iqr_k = st.sidebar.slider("IQR Sensitivity (k)", 1.0, 3.0, 1.5, 0.1)
yoy_pct = st.sidebar.slider("YoY % Threshold", 0, 100, 30, 5, format="%d%%")
yoy_thresh = yoy_pct/100.0
roll_w = st.sidebar.slider("Rolling Window (months)", 3, 12, DEFAULT_ROLL_WIN, 1)
roll_ksig = st.sidebar.slider("Rolling Sigma k", 2.0, 3.5, DEFAULT_ROLL_KSIG, 0.1)
roll_rel = st.sidebar.slider("Rolling Relative Band (%)", 10, 60, int(DEFAULT_ROLL_REL*100), 5, format="%d%%")/100.0
st.sidebar.markdown("---")
multi_year = st.sidebar.checkbox("Multi-year timeline (monthly view)", value=False)
start_year = st.sidebar.selectbox("Start Year", [2021, 2022, 2023, 2024, 2025, 2026], index=0, disabled=not multi_year)

# =========================
# Load current/prior
# =========================
sheet_name = SHEET_MAP[dataset]
df_current = load_data(current_year, sheet_name)
df_prior = load_data(comparison_year, sheet_name) if comparison_year else None
if isinstance(df_current, str):
    st.error(df_current)
    st.stop()

rq = get_reporting_quarter(current_year)
months_to_analyze = [m for m in ALL_MONTHS[:rq*3] if m in df_current.columns]

# Entity filters
entity = st.selectbox("Entity / Group", options=df_current['Entity / Group'].unique())
if 'Subquestion' in df_current.columns and df_current[df_current['Entity / Group'] == entity]['Subquestion'].nunique() > 1:
    subquestion = st.selectbox("Subquestion", options=df_current[df_current['Entity / Group'] == entity]['Subquestion'].unique())
    worker_cat = st.selectbox("Worker Category", options=df_current[(df_current['Entity / Group'] == entity) & (df_current['Subquestion'] == subquestion)]['Worker Category'].unique())
    row_cur = _row_filter(df_current, entity, worker_cat, subquestion)
else:
    subquestion = "N/A"
    worker_cat = st.selectbox("Worker Category", options=df_current[df_current['Entity / Group'] == entity]['Worker Category'].unique())
    row_cur = _row_filter(df_current, entity, worker_cat, subquestion)

row_prior = None
if not isinstance(df_prior, str) and df_prior is not None:
    row_prior = _row_filter(df_prior, entity, worker_cat, subquestion)

# Build series
if time_view == 'Monthly':
    series_cur = pd.Series(dtype=float)
    if months_to_analyze:
        series_cur = pd.to_numeric(row_cur[months_to_analyze].iloc[0], errors='coerce').astype(float)
        series_cur.index = months_to_analyze
    series_prior = None
    if row_prior is not None and not row_prior.empty and all(m in row_prior.columns for m in months_to_analyze):
        series_prior = pd.to_numeric(row_prior[months_to_analyze].iloc[0], errors='coerce').astype(float)

    # Multi-year option for exploration plot
    if multi_year and isinstance(start_year, int) and start_year <= current_year:
        vals, idx = [], []
        for yr in range(start_year, current_year+1):
            dfy = load_data(yr, sheet_name)
            if isinstance(dfy, str): continue
            ry = _row_filter(dfy, entity, worker_cat, subquestion)
            if ry.empty: continue
            qy = get_reporting_quarter(yr)
            my = [m for m in ALL_MONTHS[:qy*3] if m in ry.columns]
            if not my: continue
            sy = pd.to_numeric(ry[my].iloc[0], errors='coerce').astype(float)
            vals.extend(list(sy.values))
            idx.extend([f"{yr}-{m}" for m in my])
        if idx:
            series_cur = pd.Series(vals, index=idx, dtype=float)
            series_prior = None
else:
    series_cur = build_quarter_series(row_cur, current_year, rq)
    series_prior = None
    if row_prior is not None and not row_prior.empty and comparison_year:
        rq_prior = get_reporting_quarter(comparison_year)
        series_prior = build_quarter_series(row_prior, comparison_year, rq_prior)

# =========================
# Header KPIs
# =========================
st.markdown("### QC Intelligence Console")
colA, colB, colC, colD = st.columns(4)
with colA: st.markdown(f"<div class='kpi-card'><div class='subtle'>Current Year</div><h3>{current_year}</h3></div>", unsafe_allow_html=True)
with colB: st.markdown(f"<div class='kpi-card'><div class='subtle'>Timeframe</div><h3>{time_view}</h3></div>", unsafe_allow_html=True)
with colC: st.markdown(f"<div class='kpi-card'><div class='subtle'>Dataset</div><h3>{dataset}</h3></div>", unsafe_allow_html=True)
with colD:
    latest_val = series_cur.iloc[-1] if not series_cur.empty else np.nan
    st.markdown(f"<div class='kpi-card'><div class='subtle'>Latest Value</div><h3>{latest_val:,.0f}</h3></div>", unsafe_allow_html=True)

# =========================
# Tabs
# =========================
tab1, tab2, tab3, tab4 = st.tabs(["ðŸ”Ž Explore", "ðŸš¨ Anomalies", "ðŸ§­ Attribution", "ðŸ§¾ Report"])

# ---- Explore
with tab1:
    if series_cur.empty or series_cur.isnull().all():
        st.warning("No data for selected filters/period.")
    else:
        title = f"{'Monthly' if time_view=='Monthly' else 'Quarterly'} Trend â€” {entity} | {worker_cat}"
        plot_series(series_cur, series_prior, title)

# ---- Anomalies (Composite)
with tab2:
    if series_cur.empty or series_cur.isnull().all():
        st.info("Load a series in Explore first.")
    else:
        scores = score_pillars(
            series_cur, series_prior,
            mom_pct_thresh=0.25, mom_abs_thresh=abs_thresh,
            yoy_pct_thresh=yoy_thresh, iqr_k=iqr_k,
            roll_w=roll_w, roll_ksig=roll_ksig, roll_rel=roll_rel
        )
        comp = compose_score(scores, time_view)
        show = comp.copy()
        show['Reasons'] = show['Reasons'].apply(lambda L: ", ".join(L))
        show = show.sort_values('Composite', ascending=False)
        st.markdown("**Composite anomaly table**")
        st.dataframe(
            show.style.format({
                'Composite':'{:.2f}', 'MoM_score':'{:.1f}', 'YoY_score':'{:.1f}',
                'IQR_score':'{:.1f}', 'ROLL_score':'{:.1f}'
            }),
            use_container_width=True
        )
        csv = show.reset_index().to_csv(index=False).encode('utf-8')
        st.download_button("ðŸ“¥ Download anomalies (CSV)", csv, "anomalies.csv", "text/csv", use_container_width=True)

# ---- Attribution
with tab3:
    st.markdown("Identify **who** is driving a spike at the aggregate.")
    hmap, herr = load_hierarchy_map()
    if herr:
        st.info(herr)
    else:
        # Build a consistent slice of the current sheet across institutions for same Subquestion/Worker category
        slice_df = df_current.copy()
        if 'Subquestion' in slice_df.columns:
            slice_df = slice_df[slice_df['Subquestion'] == subquestion]
        slice_df = slice_df[slice_df['Worker Category'] == worker_cat].copy()

        if time_view == 'Monthly':
            if not months_to_analyze:
                st.warning("No months available.")
            else:
                agg_df = aggregate_monthly(slice_df, months_to_analyze)
                agg = build_level_series(agg_df, months_to_analyze, hmap)
                period_pick = st.selectbox("Pick a period to explain", months_to_analyze)
        else:
            # Build quarterly per row then widen
            periods = list(series_cur.index)
            if not periods:
                st.warning("No quarters available.")
            else:
                # Create columns for periods and fill from each rowâ€™s quarterly series
                wide = pd.DataFrame(index=slice_df['Entity / Group'].values, columns=periods, dtype=float)
                for i, r in slice_df.iterrows():
                    s = build_quarter_series(pd.DataFrame([r]), current_year, rq)
                    for p in periods:
                        wide.loc[r['Entity / Group'], p] = s.get(p, np.nan)
                # Merge back with slice_df to reuse the same aggregator
                tmp = slice_df[['Entity / Group']].drop_duplicates().copy()
                for p in periods: tmp[p] = wide[p].values
                tmp['Worker Category'] = worker_cat
                if 'Subquestion' in slice_df.columns:
                    tmp['Subquestion'] = subquestion
                agg = build_level_series(tmp, periods, hmap)
                period_pick = st.selectbox("Pick a period to explain", periods)

        if 'agg' in locals() and period_pick:
            path = attribute_path(period_pick, agg)
            if np.isnan(path.get('AllFI_delta', np.nan)):
                st.info("Not enough data to compute contributions for the chosen period.")
            else:
                st.markdown(
                    f"**Result:** High movement in **All FI** (Î” {path['AllFI_delta']:+,.0f}) "
                    f"is primarily from **{path['Sector']}** (Î” {path['Sector_delta']:+,.0f}), "
                    f"specifically **{path.get('Subsector','-')}** "
                    f"â€” **{path.get('Institution','-')}** (Î” {path['Institution_delta']:+,.0f}) at **{period_pick}**."
                )
                # Top contributors table (institutions by absolute Î”)
                inst_rows = []
                for inst, s in agg['Institution'].items():
                    d = _delta_prev(s, period_pick)
                    inst_rows.append((inst, s.get(period_pick, np.nan), d))
                top_inst = pd.DataFrame(inst_rows, columns=['Institution','Value','Î” from prev']).dropna()
                top_inst['absÎ”'] = top_inst['Î” from prev'].abs()
                top_inst = top_inst.sort_values('absÎ”', ascending=False).drop(columns='absÎ”').head(10)
                st.dataframe(top_inst, use_container_width=True)
                csv2 = top_inst.to_csv(index=False).encode('utf-8')
                st.download_button("ðŸ“¥ Download top contributions (CSV)", csv2, "attribution.csv", "text/csv", use_container_width=True)

# ---- Report (batch outliers across questions)
with tab4:
    st.markdown("Scan selected datasets and export a master report of potential outliers.")
    pick = st.multiselect("Datasets to scan", list(SHEET_MAP.keys()), default=list(SHEET_MAP.keys()))
    if st.button("ðŸš€ Run scan", use_container_width=True):
        master = []
        progress = st.progress(0, text="Scanning...")
        for i, q in enumerate(pick):
            progress.progress(i/len(pick), text=f"Scanning: {q}")
            dfc = load_data(current_year, SHEET_MAP[q])
            dfp = load_data(comparison_year, SHEET_MAP[q]) if comparison_year else None
            if isinstance(dfc, str): continue
            rq_q = get_reporting_quarter(current_year)
            months_q = [m for m in ALL_MONTHS[:rq_q*3] if m in dfc.columns]
            for _, r in dfc.iterrows():
                ent = r['Entity / Group']
                wc = r['Worker Category']
                subq = r['Subquestion'] if 'Subquestion' in dfc.columns else 'N/A'
                ser_cur = pd.to_numeric(pd.Series(r[months_q]), errors='coerce').astype(float)
                ser_cur.index = months_q
                ser_pr = None
                if isinstance(dfp, pd.DataFrame):
                    rr = dfp[(dfp['Entity / Group']==ent) & (dfp['Worker Category']==wc)]
                    if 'Subquestion' in dfp.columns:
                        rr = rr[rr['Subquestion']==subq]
                    if not rr.empty and all(m in rr.columns for m in months_q):
                        ser_pr = pd.to_numeric(rr[months_q].iloc[0], errors='coerce').astype(float)
                sc = score_pillars(
                    ser_cur, ser_pr,
                    mom_pct_thresh=0.25, mom_abs_thresh=abs_thresh,
                    yoy_pct_thresh=yoy_thresh, iqr_k=iqr_k,
                    roll_w=roll_w, roll_ksig=roll_ksig, roll_rel=roll_rel
                )
                comp = compose_score(sc, "Monthly")
                flagged = comp[comp['Label'].isin(["True Outlier","Probable Outlier"])]
                if not flagged.empty:
                    for per, row in flagged.iterrows():
                        master.append([q, ent, subq, wc, per, float(row['Composite']), row['Label'], ", ".join(row['Reasons'])])
        progress.progress(1.0, text="Done")
        if master:
            out = pd.DataFrame(master, columns=['Question','Entity / Group','Subquestion','Worker Category','Period','Composite','Label','Reasons'])
            out = out.sort_values(['Composite'], ascending=False)
            st.success(f"Found {len(out)} flagged points.")
            st.dataframe(out, use_container_width=True)
            st.download_button("ðŸ“¥ Download master report (CSV)", out.to_csv(index=False).encode('utf-8'),
                               "master_outliers.csv", "text/csv", use_container_width=True)
        else:
            st.info("No flagged outliers under current thresholds.")
